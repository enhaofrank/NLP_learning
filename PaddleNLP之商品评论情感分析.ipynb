{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 一、PaddleNLP之淘宝商品评论情感分析\n",
    "在我国电子商务飞快发展的背景下，基本上所有的电子商务网站都支持消费者对产品的做工、快递、价格个等进行打分和发表评论。在网络平台上发布大量的留言和评论，这已经成为互联网的一种流行形式，而这种形势必然给互联网带来海量的信息。\n",
    "\n",
    "**对于卖家来说**，可以从评论信息中获取客户的实际需求，以改善产品品质，提高自身的竞争力。另一方面，对于一些未知体验产品，客户可以通过网络来获取产品信息，特别是对一些未知的体验产品，客户为了降低自身的风险更加倾向于得到其他客户的意见和看法，这些评论对潜在的买家而言无疑是一笔财富，并以此作为决策的重要依据。\n",
    "\n",
    "**对于客户来说**，可以借鉴别人的购买历史以及评论信息，更好的辅助自己制定购买决策。\n",
    "\n",
    "因此，通过利用数据挖掘技术针对客户的大量评论进行分析，可以挖掘出这些信息的特征，而得到的这些信息有利于生产商改进自身产品和改善相关的服务，提高商家的核心竞争力。\n",
    "\n",
    "\n",
    "数据标签分别为 **{0: 'negative', 1: 'neutral', 2: 'positive'}**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade paddlenlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 二、数据处理\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 重写read方法读取自定义数据集\n",
    "通过查看课件，3行为一条记录，分别为评价内容、评价分类、评价正负标签，根据文件结构，自定义数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from paddlenlp.datasets import load_dataset\r\n",
    "\r\n",
    "def read(data_path):\r\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\r\n",
    "        # 跳过列名\r\n",
    "        next(f)\r\n",
    "        for line in f:\r\n",
    "            label,  word= line.strip('\\n').split('\\t')\r\n",
    "            yield {'text': word, 'label': label}\r\n",
    "\r\n",
    "# data_path为read()方法的参数\r\n",
    "train_ds = load_dataset(read, data_path='formated_train.txt',lazy=False)\r\n",
    "test_ds = load_dataset(read, data_path='formated_test.txt',lazy=False)\r\n",
    "dev_ds = load_dataset(read, data_path='formated_test.txt',lazy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13339\n",
      "None\n",
      "{'text': '质量 很棒 ！ 又 厚实 ， 就是 做工 不好 ， 就是 有点 味道 得 跑跑', 'label': '0'}\n",
      "{'text': '很好 ， 很漂亮 ， 质量 问题 就 不知道 了 ， 得用 用 才 知道 ， 这个 快递 太 垃圾 了', 'label': '0'}\n",
      "{'text': '价格 上面 反正 折扣 之后 就是 市场 销售价格 ， 搞 活动 也 就 随便说说 ， 挺好吃 的', 'label': '0'}\n",
      "{'text': '漂亮 好看 质量 好 就是 快递 不 给力 ， 派个件 要 派 两天', 'label': '0'}\n",
      "{'text': '宝贝 很 满意 ， 发小 刚刚 好 ， 质量 也 不错 ， 值得 购买 ， 就是 快递 太慢 ， 昨天 才 到', 'label': '0'}\n",
      "{'text': '行李箱 就 不错 啊 ！ 看上去 是 挺 结实 的 等 用 了 才 知道 是不是 真的 好 ！ 拉链 也 很 顺滑 ！ 快递 太差 ， 太慢', 'label': '0'}\n",
      "{'text': '质量 很好 ， 发货 很快 ， 果断 给 好评 ！ 快递 小哥 有些 烦 ， 让 他 等 我 五分钟 ， 结果 五分钟 内 打 了 三次 电话 ！', 'label': '0'}\n",
      "{'text': '颜色 很小 清新 ， 和 图片 一样 的 ， 送 的 贴纸 很 可爱 。 轮子 在 瓷砖 上 走 挺 小声 的 ， 要是 细节 处理 好 一点 就 完美 了 。 性价比 高 。 快递 没打 满分 一 是因为 不 给 送货 ， 二是 我 说 等验 完货 在 签 ， 她们 就 给 我 脸色 看 。', 'label': '0'}\n",
      "{'text': '虽然 安装 师傅 不 给力 ， 但是 东西 真的 很 给力 ！ 很 喜欢 好评 ！', 'label': '0'}\n",
      "{'text': '外观 还可以 ， 送 了 很多 贴画 ， 蛮 细心 的 。 快递 送 在 超市 门口 打个 电话 就 走 了 ， 结果 被 别人 拿走 了 ， 好歹 又 送 回来 了 & hellip ; & hellip ; 真是 糟心 ！', 'label': '0'}\n"
     ]
    }
   ],
   "source": [
    "print(len(train_ds))\r\n",
    "print(train_ds.label_list)\r\n",
    "for idx in range(10):\r\n",
    "    print(train_ds[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 三、使用预训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1.选取预训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-06-15 23:40:55,574] [    INFO] - Downloading https://paddlenlp.bj.bcebos.com/models/transformers/ernie/ernie_v1_chn_base.pdparams and saved to /home/aistudio/.paddlenlp/models/ernie-1.0\n",
      "[2021-06-15 23:40:55,577] [    INFO] - Downloading ernie_v1_chn_base.pdparams from https://paddlenlp.bj.bcebos.com/models/transformers/ernie/ernie_v1_chn_base.pdparams\n",
      "100%|██████████| 392507/392507 [00:13<00:00, 29747.03it/s]\n",
      "[2021-06-15 23:41:15,870] [    INFO] - Weights from pretrained model not used in ErnieModel: ['cls.predictions.layer_norm.weight', 'cls.predictions.decoder_bias', 'cls.predictions.transform.bias', 'cls.predictions.transform.weight', 'cls.predictions.layer_norm.bias']\n",
      "[2021-06-15 23:41:16,263] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/ernie-1.0/ernie_v1_chn_base.pdparams\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1297: UserWarning: Skip loading for classifier.weight. classifier.weight is not found in the provided dict.\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1297: UserWarning: Skip loading for classifier.bias. classifier.bias is not found in the provided dict.\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\n"
     ]
    }
   ],
   "source": [
    "import paddlenlp as ppnlp\r\n",
    "\r\n",
    "# 设置想要使用模型的名称\r\n",
    "MODEL_NAME = \"ernie-1.0\"\r\n",
    "ernie_model  = ppnlp.transformers.ErnieModel.from_pretrained(MODEL_NAME)\r\n",
    "model = ppnlp.transformers.ErnieForSequenceClassification.from_pretrained(MODEL_NAME, num_classes=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.调用ppnlp.transformers.ErnieTokenizer进行数据处理\n",
    "\n",
    "预训练模型ERNIE对中文数据的处理是以字为单位。PaddleNLP对于各种预训练模型已经内置了相应的tokenizer。指定想要使用的模型名字即可加载对应的tokenizer。\n",
    "\n",
    "tokenizer作用为将原始输入文本转化成模型model可以接受的输入数据形式。\n",
    "\n",
    "![](https://bj.bcebos.com/paddlehub/paddlehub-img/ernie_network_1.png)\n",
    "\n",
    "![](https://bj.bcebos.com/paddlehub/paddlehub-img/ernie_network_2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-06-15 23:41:18,286] [    INFO] - Downloading vocab.txt from https://paddlenlp.bj.bcebos.com/models/transformers/ernie/vocab.txt\n",
      "100%|██████████| 90/90 [00:00<00:00, 3163.87it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = ppnlp.transformers.ErnieTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "从以上代码可以看出，ERNIE模型输出有2个tensor。\n",
    "\n",
    "* sequence_output是对应每个输入token的语义特征表示，shape为(1, num_tokens, hidden_size)。其一般用于序列标注、问答等任务。\n",
    "* pooled_output是对应整个句子的语义特征表示，shape为(1, hidden_size)。其一般用于文本分类、信息检索等任务。\\\n",
    "\n",
    "NOTE:\n",
    "\n",
    "* 如需使用ernie-tiny预训练模型，则对应的tokenizer应该使用paddlenlp.transformers.ErnieTinyTokenizer.from_pretrained('ernie-tiny')\n",
    "* \n",
    "* 以上代码示例展示了使用Transformer类预训练模型所需的数据处理步骤。为了更方便地使用，PaddleNLP同时提供了更加高阶API，一键即可返回模型所需数据格式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3.数据读取\n",
    "使用paddle.io.DataLoader接口多线程异步加载数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from functools import partial\r\n",
    "from paddlenlp.data import Stack, Tuple, Pad\r\n",
    "from utils import  convert_example, create_dataloader\r\n",
    "\r\n",
    "# 模型运行批处理大小\r\n",
    "batch_size = 200\r\n",
    "max_seq_length = 128\r\n",
    "\r\n",
    "trans_func = partial(\r\n",
    "    convert_example,\r\n",
    "    tokenizer=tokenizer,\r\n",
    "    max_seq_length=max_seq_length)\r\n",
    "batchify_fn = lambda samples, fn=Tuple(\r\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input\r\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # segment\r\n",
    "    Stack(dtype=\"int64\")  # label\r\n",
    "): [data for data in fn(samples)]\r\n",
    "train_data_loader = create_dataloader(\r\n",
    "    train_ds,\r\n",
    "    mode='train',\r\n",
    "    batch_size=batch_size,\r\n",
    "    batchify_fn=batchify_fn,\r\n",
    "    trans_fn=trans_func)\r\n",
    "dev_data_loader = create_dataloader(\r\n",
    "    dev_ds,\r\n",
    "    mode='dev',\r\n",
    "    batch_size=batch_size,\r\n",
    "    batchify_fn=batchify_fn,\r\n",
    "    trans_fn=trans_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 4.设置Fine-Tune优化策略，接入评价指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from paddlenlp.transformers import LinearDecayWithWarmup\r\n",
    "import paddle\r\n",
    "\r\n",
    "# 训练过程中的最大学习率\r\n",
    "learning_rate = 5e-5 \r\n",
    "# 训练轮次\r\n",
    "epochs = 5 #3\r\n",
    "# 学习率预热比例\r\n",
    "warmup_proportion = 0.1\r\n",
    "# 权重衰减系数，类似模型正则项策略，避免模型过拟合\r\n",
    "weight_decay = 0.01\r\n",
    "\r\n",
    "num_training_steps = len(train_data_loader) * epochs\r\n",
    "lr_scheduler = LinearDecayWithWarmup(learning_rate, num_training_steps, warmup_proportion)\r\n",
    "optimizer = paddle.optimizer.AdamW(\r\n",
    "    learning_rate=lr_scheduler,\r\n",
    "    parameters=model.parameters(),\r\n",
    "    weight_decay=weight_decay,\r\n",
    "    apply_decay_param_fun=lambda x: x in [\r\n",
    "        p.name for n, p in model.named_parameters()\r\n",
    "        if not any(nd in n for nd in [\"bias\", \"norm\"])\r\n",
    "    ])\r\n",
    "\r\n",
    "criterion = paddle.nn.loss.CrossEntropyLoss()\r\n",
    "metric = paddle.metric.Accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 四、模型训练与评估\n",
    "模型训练的过程通常有以下步骤：\n",
    "\n",
    "1. 从dataloader中取出一个batch data\n",
    "2. 将batch data喂给model，做前向计算\n",
    "3. 将前向计算结果传给损失函数，计算loss。将前向计算结果传给评价方法，计算评价指标。\n",
    "4. loss反向回传，更新梯度。重复以上步骤。\n",
    "\n",
    "每训练一个epoch时，程序将会评估一次，评估当前模型训练的效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/home/aistudio/checkpoint’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "# checkpoint文件夹用于保存训练模型\r\n",
    "!mkdir /home/aistudio/checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle.nn.functional as F\r\n",
    "from utils import evaluate\r\n",
    "\r\n",
    "global_step = 0\r\n",
    "for epoch in range(1, epochs + 1):\r\n",
    "    for step, batch in enumerate(train_data_loader, start=1):\r\n",
    "        input_ids, segment_ids, labels = batch\r\n",
    "        logits = model(input_ids, segment_ids)\r\n",
    "        loss = criterion(logits, labels)\r\n",
    "        probs = F.softmax(logits, axis=1)\r\n",
    "        correct = metric.compute(probs, labels)\r\n",
    "        metric.update(correct)\r\n",
    "        acc = metric.accumulate()\r\n",
    "\r\n",
    "        global_step += 1\r\n",
    "        if global_step % 10 == 0 :\r\n",
    "            print(\"global step %d, epoch: %d, batch: %d, loss: %.5f, acc: %.5f\" % (global_step, epoch, step, loss, acc))\r\n",
    "        loss.backward()\r\n",
    "        optimizer.step()\r\n",
    "        lr_scheduler.step()\r\n",
    "        optimizer.clear_grad()\r\n",
    "    evaluate(model, criterion, metric, dev_data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "```\n",
    "global step 270, epoch: 5, batch: 2, loss: 0.43869, acc: 0.80500\n",
    "global step 280, epoch: 5, batch: 12, loss: 0.41893, acc: 0.80792\n",
    "global step 290, epoch: 5, batch: 22, loss: 0.44744, acc: 0.80523\n",
    "global step 300, epoch: 5, batch: 32, loss: 0.38977, acc: 0.80797\n",
    "global step 310, epoch: 5, batch: 42, loss: 0.39923, acc: 0.80774\n",
    "global step 320, epoch: 5, batch: 52, loss: 0.36724, acc: 0.81029\n",
    "global step 330, epoch: 5, batch: 62, loss: 0.40811, acc: 0.81008\n",
    "eval loss: 0.51191, accu: 0.74444\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 五、模型预测\n",
    "训练保存好的训练，即可用于预测。如以下示例代码自定义预测数据，调用predict()函数即可一键预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: {'text': '这个宾馆比较陈旧了，特价的房间也很一般。总体来说一般'} \t Lable: neutral\n",
      "Data: {'text': '怀着十分激动的心情放映，可是看着看着发现，在放映完毕后，出现一集米老鼠的动画片'} \t Lable: negative\n",
      "Data: {'text': '作为老的四星酒店，房间依然很整洁，相当不错。机场接机服务很好，可以在车上办理入住手续，节省时间。'} \t Lable: positive\n"
     ]
    }
   ],
   "source": [
    "from utils import predict\r\n",
    "\r\n",
    "data = [\r\n",
    "    {\"text\":'这个宾馆比较陈旧了，特价的房间也很一般。总体来说一般'},\r\n",
    "    {\"text\":'怀着十分激动的心情放映，可是看着看着发现，在放映完毕后，出现一集米老鼠的动画片'},\r\n",
    "    {\"text\":'作为老的四星酒店，房间依然很整洁，相当不错。机场接机服务很好，可以在车上办理入住手续，节省时间。'},\r\n",
    "]\r\n",
    "label_map = {0: 'negative', 1: 'neutral', 2: 'positive'}\r\n",
    "\r\n",
    "results = predict(\r\n",
    "    model, data, tokenizer, label_map, batch_size=batch_size)\r\n",
    "for idx, text in enumerate(data):\r\n",
    "    print('Data: {} \\t Lable: {}'.format(text, results[idx]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.1.0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
