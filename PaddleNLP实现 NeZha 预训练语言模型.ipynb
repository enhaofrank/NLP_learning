{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PaddleNLP：实现 NeZha 预训练语言模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **相关资料**\n",
    "* 论文：[NEZHA: Neural Contextualized Representation for Chinese Language Understanding](https://arxiv.org/abs/1909.00204)\n",
    "* 官方实现：[huawei-noah/Pretrained-Language-Model](https://github.com/huawei-noah/Pretrained-Language-Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **主要改进**\n",
    "* 函数式相对位置编码（Functional Relative Positional Encoding）\n",
    "* 全词掩码训练（Whole Word Masking）\n",
    "* 混合精度训练（Automatic Mixed Precision）\n",
    "* 训练过程中使用 LAMB 优化器（Layer-wise Adaptive Moments optimizer）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **文本分类任务微调**\n",
    "* 这里就使用经典的文本分类任务来演示一下模型微调的操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddlenlp as ppnlp\n",
    "import paddle.nn.functional as F\n",
    "\n",
    "from paddlenlp.datasets import load_dataset\n",
    "from paddlenlp.data import Stack, Tuple, Pad\n",
    "from paddlenlp.transformers import LinearDecayWithWarmup\n",
    "\n",
    "from functools import partial\n",
    "from utils import convert_example, create_dataloader, evaluate\n",
    "from nezha import NeZhaForSequenceClassification, NeZhaTokenizer\n",
    "\n",
    "\n",
    "train_ds, dev_ds, test_ds = load_dataset(\n",
    "    \"chnsenticorp\", \n",
    "    splits=[\"train\", \"dev\", \"test\"]\n",
    ")\n",
    "\n",
    "model = NeZhaForSequenceClassification.from_pretrained(\n",
    "    'nezha-base-chinese', \n",
    "    num_classes=len(train_ds.label_list)\n",
    ")\n",
    "\n",
    "tokenizer = NeZhaTokenizer.from_pretrained('nezha-base-chinese')\n",
    "\n",
    "\n",
    "# 模型运行批处理大小\n",
    "batch_size = 32\n",
    "max_seq_length = 128\n",
    "\n",
    "trans_func = partial(\n",
    "    convert_example,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=max_seq_length)\n",
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # segment\n",
    "    Stack(dtype=\"int64\")  # label\n",
    "): [data for data in fn(samples)]\n",
    "train_data_loader = create_dataloader(\n",
    "    train_ds,\n",
    "    mode='train',\n",
    "    batch_size=batch_size,\n",
    "    batchify_fn=batchify_fn,\n",
    "    trans_fn=trans_func)\n",
    "dev_data_loader = create_dataloader(\n",
    "    dev_ds,\n",
    "    mode='dev',\n",
    "    batch_size=batch_size,\n",
    "    batchify_fn=batchify_fn,\n",
    "    trans_fn=trans_func)\n",
    "test_data_loader = create_dataloader(\n",
    "    test_ds,\n",
    "    mode='test',\n",
    "    batch_size=batch_size,\n",
    "    batchify_fn=batchify_fn,\n",
    "    trans_fn=trans_func)\n",
    "\n",
    "\n",
    "# 训练过程中的最大学习率\n",
    "learning_rate = 5e-5 \n",
    "# 训练轮次\n",
    "epochs = 3\n",
    "# 学习率预热比例\n",
    "warmup_proportion = 0.1\n",
    "# 权重衰减系数，类似模型正则项策略，避免模型过拟合\n",
    "weight_decay = 0.01\n",
    "\n",
    "num_training_steps = len(train_data_loader) * epochs\n",
    "lr_scheduler = LinearDecayWithWarmup(learning_rate, num_training_steps, warmup_proportion)\n",
    "optimizer = paddle.optimizer.AdamW(\n",
    "    learning_rate=lr_scheduler,\n",
    "    parameters=model.parameters(),\n",
    "    weight_decay=weight_decay,\n",
    "    apply_decay_param_fun=lambda x: x in [\n",
    "        p.name for n, p in model.named_parameters()\n",
    "        if not any(nd in n for nd in [\"bias\", \"norm\"])\n",
    "    ])\n",
    "\n",
    "criterion = paddle.nn.loss.CrossEntropyLoss()\n",
    "metric = paddle.metric.Accuracy()\n",
    "\n",
    "global_step = 0\n",
    "for epoch in range(1, epochs + 1):\n",
    "    for step, batch in enumerate(train_data_loader, start=1):\n",
    "        input_ids, segment_ids, labels = batch\n",
    "        logits = model(input_ids, segment_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        probs = F.softmax(logits, axis=1)\n",
    "        correct = metric.compute(probs, labels)\n",
    "        metric.update(correct)\n",
    "        acc = metric.accumulate()\n",
    "\n",
    "        global_step += 1\n",
    "        if global_step % 10 == 0 :\n",
    "            print(\"global step %d, epoch: %d, batch: %d, loss: %.5f, acc: %.5f\" % (global_step, epoch, step, loss, acc))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.clear_grad()\n",
    "    evaluate(model, criterion, metric, dev_data_loader)\n",
    "\n",
    "model.save_pretrained('/home/aistudio/checkpoint')\n",
    "tokenizer.save_pretrained('/home/aistudio/checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#效果测试\n",
    "from utils import predict\n",
    "\n",
    "data = [\n",
    "    {\"text\":'这个宾馆比较陈旧了，特价的房间也很一般。总体来说一般'},\n",
    "    {\"text\":'怀着十分激动的心情放映，可是看着看着发现，在放映完毕后，出现一集米老鼠的动画片'},\n",
    "    {\"text\":'作为老的四星酒店，房间依然很整洁，相当不错。机场接机服务很好，可以在车上办理入住手续，节省时间。'},\n",
    "]\n",
    "label_map = {0: 'negative', 1: 'positive'}\n",
    "\n",
    "results = predict(\n",
    "    model, data, tokenizer, label_map, batch_size=batch_size)\n",
    "for idx, text in enumerate(data):\n",
    "    print('Data: {} \\t Lable: {}'.format(text, results[idx]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
